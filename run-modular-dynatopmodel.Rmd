---
title: "Run Modular Dynatopmodel"
output: html_notebook
---

# Step 1: Read in required libraries

```{r}
library(dynatopmodel)
library(sp)
library(raster)
library(reticulate)
library(topmodel)
library(ggplot2)
library(mapview)
library(hydroPSO)                                               # Read in hydroPSO algorithm
library(hydroGOF)                                               # Read in hydroGOF package (goodness of fit)
library(hydroTSM)                                               # Read in hydroTSM package
library(boot)                                                   # Read in boot package
library(zoo)                                                    # Read in zoo package for time series analysis 
library(sensitivity)                                            # Read in the sensitivity analysis package
library(xts)                                                    # Read in the xts package also for time series 
library(ggplot2)                                                # Read in ggplot2 package for figure 
library(dplyr)                                                  # Read in dplyr package
library(readr)                                                  # Read in read r package
library(lubridate)                                              # Read in lubridate for date manipulation
library(Evapotranspiration)                                     # Read in the Evapotranspiration packahge 
library(rasterVis)                                              # Read in the rasterVis package for raster 
library(dismo)                                                  # Read in dismo package for k fold cross
library(rgl)                                                    # Read in the rgl package for 3D scatter plots
library(matrixStats)                                            # Read in the library for matrixStats
library(gridExtra)                                              # Read in the gridExtra package for GGPLOT2
library(tidyquant)

```

# Step 2: Load in Spatial Data 
Note, this should be saved from the 'run-modular-discretization.Rmd' Notebook 

```{r}
# Read in the Spatial data 
read.spatial <- readRDS('SpatialInputData/dynatop_spatial_LM.RData')
#read.spatial.FR <- readRDS('SpatialInputData/dynatop_spatial_FR.RData')
disc <- read.spatial$disc                                      
RoutingTable <- read.spatial$RoutingTable                      
explicit.reach.table <- read.spatial$explicit.ChanTable         

```

# Step 3: Read in the hydrometeorological data 
Note, this should be saved from the 'run-modular-hydro-data.Rmd' Notebook. 
Reminder: if any functions are used to develop the time series that are created within, they must be loaded prior to running the chunk. 

```{r}
# Read in the hydromet data
read.hydromet <- readRDS('HydroInputData/dynatop_hydromet_LM.RData')

input.timeseries <- read.hydromet$input.timeseries

# Specify the Model Time Step (and internal time step)
model.timestep <- read.hydromet$dt        #1  #2                                 # This should match the dt specified from run-modular-hydro-data
in.timestep <- read.hydromet$dt           #1  #2                                 # Specify the number of internal time steps (unitless) - this is for the internal loop to calculate the baseflow

# Specify Calibration Period
warmup.initial <- read.hydromet$warmup.initial
warmup.final <- read.hydromet$warmup.final

calibration.initial <- read.hydromet$calibration.initial
calibration.final <- read.hydromet$calibration.final

# Note: it's important to format the timeseries in terms of UTC otherwise timesteps will be added (annoyingly)

# Concatinate the dates
dates.df <- data.frame(calibration.initial, calibration.final,  # Concatinate the dates into a df 
                       warmup.initial, warmup.final)            # Continued
names.dates <- c('calibration.initial','calibration.final',     # Create names for the df
                 'warmup.initial','warmup.final')               # Continued
names(dates.df) <- names.dates                                  # Specify the date names 

# Specify the watershed area
watershed.area <- sum(disc$groups$area)                         # Specify the watershed area (m^2)

# Load in the data from the input timeseries from the run-modular-hydro-data file
# Currently no validation period is specified 

rain.calib <- input.timeseries$P[1:(length(input.timeseries$P))]                     # Writing the P TS to a variable
PET.calib <- input.timeseries$PET[1:(length(input.timeseries$PET))]                  # Writing the PET TS to a variable
Q.obs.calib <- input.timeseries$Qobs[1:(length(input.timeseries$Qobs))]              # Writing the Qobs TS to a variable
dates.cal.warm <- time(rain.calib)  
```

# Step 4: Set up the parameter values and load in the functions to run the model
```{r}

source('run-modular-dynatopmodel-functions.R')

# Set up the model parameters; Read in min and max values (these values should be based on the literature).
v.of.min <- 10; v.of.max <- 150                                 # Overland flow velocity (m/hr) min and max
m.min <- -9.908; m.max <- -2.813                                # Form of exponential decline in conductivity (m) min and max
srz.max.min <-  .01; srz.max.max <- .75                         # Max root zone storage (m) min and max
srz.0.min <- 0.5; srz.0.max <- 1                                # Initial root zone storage (fraction) min and max
v.chan.min <- 500; v.chan.max <- 7000                           # Channel routing velocity (m/hr) min and max
natlog.T0.min <- 3; natlog.T0.max <- 16                         # Lat saturated transmissivity (m^2/hr) min and max
sd.max.min <- 0.2; sd.max.max <- 0.8                            # Max effective deficit of sat zone (m) min and max
td.min <- 0.01; td.max <- 100                                   # Unsat zone time delay (hr/m) min and max
mann.n.min <- 0.01; mann.n.max <- .15                           # Manning's (unitless) n min and max
S0.min <- 0.01; S0.max <- 0.3                                   # Nominal slope (fraction) min and max
CD.min <-  0.01; CD.max <- 0.5                                  # Capilary drive (unused) min and max
k0.min <-  0.1; k0.max <- 1000                                  # Initial saturated hydraulic conductivity (m/hr; unused) min and max
m1.min <- -9.908; m1.max <- -2.813                              # Assign range for the first m     
m2.min <- -9.908; m2.max <- -2.813                              # Assign range for the second m
m3.min <- -9.908; m3.max <- -2.813                              # Assign range for the third m 

# Concatinate parameter minmum, maximum, and names. Assign names to vectors
lower <- c(v.of.min, m.min, srz.max.min, srz.0.min, v.chan.min, # Vector of lower parametr values
           natlog.T0.min, sd.max.min, td.min,                   # Will be used to generate parameter sets 
           mann.n.min, S0.min, CD.min, k0.min,                  # Continued
           m1.min, m2.min, m3.min)                              # During calibration

upper <- c(v.of.max, m.max, srz.max.max, srz.0.max, v.chan.max, # Vector of upper parameter values
           natlog.T0.max, sd.max.max, td.max,                   # Will be used to generate parmeter sets 
           mann.n.max, S0.max, CD.max, k0.max,                  # Continued 
           m1.max, m2.max, m3.max)                              # During Calibration 

name.params <- c('v.of', 'm','srz.max','srz.0',                 # Create names of all parameters
                 'v.chan','natlog.T0','sd.max',                 # Included in Dynatopmodel
                 'td','mann.n','S0','CD','k0',                  # Continued
                 'm1','m2','m3')                                # To be implemented within the uncertainty and calibration
names(lower) <- name.params                                     # Specify names of columns for the lower part of the parameter space
names(upper) <- name.params                                     # Specify names of columns for the upper part of the parameter space

params <- lower+(upper-lower)/2                                 # Unused - but for a test run of the model

params.best.FC4 <- c(0.978,-6.75, 0.679,0.981,500,5.76,.606,1.58,0.0376,.181,.136,.0693,-9.91,-3.22,-2.81)

```

# Step 5: Run the model with random values and the optimal parameterziation from the previous model

Note: I'm not expecting for the calibration statistics for either model to be particularly good initially since this is a different spatial parameterization... but we'll see. 

```{r}


# Run a test of the model to make sure that things look good before running the PSO calibration 
result.test <- runPSOCalibrationTopmodel(param.values=params,                      # TEST - Run the model with the optimal parameter set
                                         inner.timesteps = in.timestep,            # input the inner.timesteps
                                         rains = rain.calib, PETs=PET.calib,       # All forcing components are the same
                                         obss=Q.obs.calib, discs=disc,          # Spatial information comes from the dynatop spatial function
                                         RoutingTables=NULL,                       # See above
                                         dates.dfs = dates.df)                     # Calibration end date input
result.test.FC4 <- runPSOCalibrationTopmodel(param.values=params.best.FC4,                      # TEST - Run the model with the optimal parameter set
                                             inner.timesteps = in.timestep,            # input the inner.timesteps
                                             rains = rain.calib, PETs=PET.calib,       # All forcing components are the same
                                             obss=Q.obs.calib, discs=disc,          # Spatial information comes from the dynatop spatial function
                                             RoutingTables=NULL,                       # See above
                                             dates.dfs = dates.df)                     # Calibration end date input

out.test <- data.frame(sim=result.test.FC4$sim,obs=result.test.FC4$run$qobs)
#write.csv(out.test,'out-test-LM.csv')
```

# Step 6: Calibrate the model for discharge at the outlet of the watershed
```{r}
source('run-modular-dynatopmodel-functions.R')

# Define the directory where the data will be saved to. 
model.drty <- 'D:/Data/Dynamic-TOPMODEL/Little-Millseat/Calibration-1'

# Create a file for the figures to be pasted if it is needed.
Figures.drty.out <- paste0(model.drty, "/Figures")                                 # Create directroy to store the figures for the calibration
if (!file.exists(Figures.drty.out)) dir.create(Figures.drty.out,                   # Creates a figure directory
                                               recursive=TRUE)                     # If the output directory selected to store the figures does not exists, it is created:

# Parameterize the routing table for the model
RoutingTables <- NULL                                                              # Note: as of now since FR is so small, I am assuming that the routing table is null, meaning that there is no attenuation really. This assumption is okay for now given the small size of the catchment.
RoutingTable <- NULL                                                               # Note: as of now since FR is so small, I am assuming that the routing table is null, meaning that there is no attenuation really. This assumption is okay for now given the small size of the catchment.

# Run the PSO calibration procedure: Note we were having issues running the model with the model.FUN.args as the method to read the arguments. 
# NOTE: Now we run the hydroPSO algorithm with the model arguments as 'global' variables.
out <- hydroPSO(fn="hydromodInR",                               # This is an optional, but necessary specification that the calibration approach is a 'hydromod' with a model as an outside function
                lower=lower,                                    # Input the lower parameter range
                upper=upper,                                    # Input the upper parameter range
                method="spso2011",                              # Specify the algorithm to calibrate the model
                control=list(write2disk=TRUE, MinMax="max",     # These are the parameters for the HydroPSO calibration approach
                             npart=10, maxit=8, normalise=TRUE, # See the documentation for each of these items # Before i had at npart = 80 and maxit =25; the typical runs were 50 npart * 5 iter # before 12/1/2022 npart = 55, maxit = 9, report =9. I needed to cut this down for time. 
                             REPORT=8, parallel="none",        # The model can either be run a certain number of iterations or it can reach a cut off threshold
                             reltol=1E-10),                     # There are also coefficients for how widely the search space is and another coefficient to control how quickly convergence occurs (Not shown, these are currently defaults)
                model.FUN="Dynatophydromod",                    # This is the function that calls dynatop
                model.FUN.args= list(obs = Q.obs.calib,                            # Timeseries of observed Q data to compare the simulated results #inner.timestep=in.timestep,            # Number of inner timesteps
                                     inner.timestep = in.timestep,                 # Inner timestep
                                     rain = rain.calib,                            # Timeseries of precipitation at dt to run the model 
                                     PET = PET.calib,                              # Timeseries of PET at dt needed to run the model 
                                     disc = disc,                               # Spatial discretization (weighting matrix) and 'groups' parameter matrix 
                                     RoutingTable = NULL,                          # Routing table to discretize stream network
                                     date = dates.cal.warm,                        # Vector of dates for the calibration period
                                     dates.df = dates.df,                          # Date frame of dates used for warm up and calibration periods
                                     model.drty=model.drty),                       # Input the model directory                              
                obs = Q.obs.calib,                              # Note above we're the model.FUN.args wasn't working properly, so we're adding this back in as variables that will feed into the model
                inner.timestep = in.timestep,                   # Inner timestep   
                rain = rain.calib,                              # Timeseries of precipitation at dt to run the model 
                PET = PET.calib,                                # Timeseries of PET at dt needed to run the model
                disc = disc,                                 # Spatial discretization (weighting matrix) and 'groups' parameter matrix 
                RoutingTable = NULL,                            # Routing table to discretize stream network
                date = dates.cal.warm,                          # Array of dates for each time step
                dates.df = dates.df,                            # Date frame of dates used for warm up and calibration periods
                model.drty = model.drty,                        # Model directory    
                # Date frame of dates used for warm up and calibration periods       
)

saveRDS(out,'D:/Data/Dynamic-TOPMODEL/Little-Millseat/Calibration-1/dynatop_calib_LM.RData')
```
# Step 7: Conduct post-processing to route the water thruough the stream network. 

```{r}
test.run <- result.test.FC4$run

out.routing <- explicit.routing.instant(read.spatial=read.spatial,explicit.reach.table=explicit.reach.table,run=test.run,model.timestep=model.timestep)


```
# Step 8: Run the post-processing code to estimate the subsurface transmissivity that best predicts the flow state in our three reaches 

```{r}
model.drty <- 'D:/Data/Dynamic-TOPMODEL/Little-Millseat/Calibration-1'

qin.file <- 1

TS.logger.clean.all <- read.hydromet$TS.logger.clean.all

TS.zoo.all <- read.hydromet$TS.zoo.all

reach.identifiers <- read.hydromet$reach.loggers$Reach

time.sim <- time(Q.obs.calib)

test.HW.eval <- Headwater.evaluation.dynamic(model.drty,qin.file, read.spatial, TS.logger.clean.all, TS.zoo.all, reach.identifiers, model.timestep, time.sim)


# Function 5: Fucntion to evaluate each behavioral parameterization and simulate streamflow permanence  
Headwater.evaluation.dynamic <- function(model.drty,qin.file, read.spatial, TS.logger.clean.all, TS.zoo.all, reach.identifiers, model.timestep, time.sim, ...) {
  # Read in explicit HRU results for fluxes and storages from the behavioral sets
  # NOTE WE MUST SET THE DIRECTORY TO THE PROPER CALIBRATION TEST FOLDER, AS OF 7/6/2021 IT IS CalibrationTest13
  fluxes.stores.dir <- paste0(model.drty,'/fluxes_stores/')
  #setwd('C:/Users/david/OneDrive/Desktop/EPA/EPA/6 PROJECT 1 KENTUCKY HEADWATER STREAMS/4 ANALYSIS/2 DYNAMIC TOPMODEL ANALYSIS/1 FR Test/Calibration/CalibrationTest13/fluxes_stores')
  qin.run <- read.csv(paste0(fluxes.stores.dir,'fluxqin',qin.file,'.csv'))                           # Read qin 
  qin.run <- xts(qin.run[,-1],time.sim)                                            # Convert to xts
  qbf.run <- read.csv(paste0(fluxes.stores.dir,'fluxqbf',qin.file,'.csv'))                           # Read qbf
  qbf.run <- xts(qbf.run[,-1],time.sim)                                            # Convert to xts
  ae.run <- read.csv(paste0(fluxes.stores.dir,'fluxae',qin.file,'.csv'))                             # Read ae
  ae.run <- xts(ae.run[,-1],time.sim)                                              # Convert to xts
  rain.run <- read.csv(paste0(fluxes.stores.dir,'fluxrain',qin.file,'.csv'))                         # Read rain
  rain.run <- xts(rain.run[,-1],time.sim)                                          # Convert to xts
  qof.run <- read.csv(paste0(fluxes.stores.dir,'fluxqof',qin.file,'.csv'))                           # Read qof
  qof.run <- xts(qof.run[,-1],time.sim)                                            # Convert to xts
  Qsim.run <- read.csv(paste0(fluxes.stores.dir,'Q_sim_big',qin.file,'.csv'))                             # Read Qsim
  Qsim.run <- xts(Qsim.run[,-1],time.sim)                                          # Convert to xts
  compile.fluxes <- list(qin = qin.run, qbf = qbf.run, ae = ae.run,                # Compile fluxes 
                         rain = rain.run, qof = qof.run)                           # Into list
  compile.run <- list(fluxes=compile.fluxes, Qsim = Qsim.run)                      # Compile fluxes and Qsim into list
  
  # Run routing function -- note TOPMODEL puts out data in m/hr for reach-specific flow in runoff
  explicit.routing.run <- explicit.routing.instant(read.spatial,                     # Read in read.spatial and the compiled run
                                                   explicit.reach.table = explicit.reach.table,
                                                   run=compile.run,
                                                   model.timestep = model.timestep)# Run the explicit routing code for the simulation
  
  Q <- data.frame(explicit.routing.run$routed.flow.kinematic.mm_s)                   # Assign the routed flow instant (mm/s) to a variables
  r.names <- names(Q)                                                              # Get the reach names
  
  # Convert Q from mm/s to mm/day
  Q <- Q*60*60*24                                                                  # This is important for some of the empirical calculations we'll later need to do
  
  # Calculate the fdc of each reach 
  Flow.duration.reaches <- matrix(nrow = length(Q[,1]),ncol = length(r.names))   # initialize the flow duration matrix
  Q.in.rank <- matrix(nrow = length(Q[,1]),ncol = length(r.names))               # Initialize the Q in rank matrix
  colnames(Flow.duration.reaches) <- r.names                                       # Assign names to the flow duration matrix
  colnames(Q.in.rank) <- r.names                                                   # Assign names to the Q in rank matrix
  
  # Run the FDC code for the reaches
  for (r.name in 1:length(r.names)) {                                              # For each of the reaches
    
    Q.in <- Q[,r.name]                                                           # Assing the Q in for a reach
    order <- explicit.reach.table[r.name,6]                                      # Get the stream order
    Q.in.sort <- data.frame(flow.asc=sort(Q.in,decreasing =T))                   # Rank/sort the Q in
    rank <- 1:length(Q.in)                                                       # Get the rank
    Q.in.sort$rank <- rank                                                       # assign the rank for the sorted data dataframe
    Q.in.sort$Prob.Q.in <- 100*(Q.in.sort$rank/(length(Q.in)+1))                 # Calculate the probablility
    Flow.duration.reaches[,paste0('R',as.character(explicit.reach.table$link_no[r.name]))] <- Q.in.sort$Prob.Q.in                                # Record the probability 
    Q.in.rank[,paste0('R',as.character(explicit.reach.table$link_no[r.name]))] <- Q.in.sort$flow.asc                                             # Record the ascending flow
    
  }
  Flow.duration.reaches <- data.frame(Flow.duration.reaches)                       # Write flow duration as a dataframe
  Q.in.rank <- data.frame(Q.in.rank)                                               # Write flow duration as a dataframe  
  
  # Write a function that returns the flow associated with the flow threshold percentage desired by the user
  nearest <- function(want,have) {                                                 # Function's name is nearest, two arguments in
    near <- which(abs(want-have)==min(abs(want-have)))                             # Return the percentage that's nearest to the desired percentage threshold
    return(near)                                                                   # Return the percentage
  }
  
  x=Sys.time()
  ## Run the analysis for 1000 flow thresholds, output the optimum flow threshold. 
  min.threshold <- 0.05                                                           # This was the minimum transmissivity scaling exponent measured from the Prancevic and Kirchner 2018 Paper
  max.threshold <- 20                                                             # This was the maximum transmissivity scaling exponent measured from the Prancevic and Kirchner 2018 Paper
  gamma.range <- seq(from=log(min.threshold), to = log(max.threshold),                       # Creates a sequency of 1000 thresholds for which we'll calculate the % correct
                     by = (log(max.threshold)-log(min.threshold))/1000)                 # Continued 
  
  # Format the data 
  names.loggers <- names(TS.logger.clean.all)
  
  # Initialize the lists for each of the loggers. 
  logger.TS <- list()                                                              # Initialize the logger Timeseries list
  logger.timeseries <- list()
  sim.obs.difference <- list()
  confusion.matrix.thresh <- list()
  out.thresh.df <- list()
  reach.slope <- data.frame(matrix(nrow=length(names.loggers)))
  reach.contributing.area <- data.frame(matrix(nrow=length(names.loggers)))
  
      # Calculate the respective error for each flow thresh for each sensor
  residual.on.off <- matrix(nrow=length(gamma.range),ncol=length(names.loggers))
  
  for (name.logger in 1:length(names.loggers)) {
    # Format the time series properly
    #time.sim <- time(result.optim.run$run$fluxes$qin)                                # Get the time stamp for the qsim
    time.sim <- data.frame(Date=time.sim)                                            # Set the col name to 'Date'
    logger.TS[[name.logger]] <- data.frame(state=TS.zoo.all[[name.logger]],
                                           Date=TS.logger.clean.all[[name.logger]]$Date)                  # Get the logger time series
    
    # Join the logger.TS to the time.sim 
    logger.timeseries[[name.logger]] <- left_join(time.sim,logger.TS[[name.logger]],by='Date')                                    # Join the logger data to the time series
    logger.timeseries[[name.logger]]$state <- ifelse(logger.timeseries[[name.logger]]$state==3,NA,logger.timeseries[[name.logger]]$state)        # Set NA values
    logger.timeseries[[name.logger]] <- logger.timeseries[[name.logger]][complete.cases(logger.timeseries[[name.logger]]),]                      # Remove rows with NA values
    
    # initialize the matrix that records the difference in simulated and observed
    sim.obs.difference[[name.logger]] <- data.frame(
      matrix(nrow=length(logger.timeseries[[name.logger]]$Date),ncol=length(gamma.range)+1))
    
    # Get the contributing area and slope for the reach 
    reach.name.dynamic <- which(                                                 # Determine which row the reach name of the logger belongs to
      read.spatial$explicit.ChanTable$link_no==                                    # Compare the list of reach numbers to the current logger being evaluated
        as.numeric(reach.identifiers[name.logger]))                  # Define the reach name as a numeric
    reach.slope[name.logger,1] <-                                                  # Determine the slope of the reach (ft/ft) or (m/m)
      read.spatial$explicit.ChanTable$stream_slope_ftpft[reach.name.dynamic]       # Using the which function from above
    reach.contributing.area[name.logger,1] <-                                      # Determine the contributing area (m^2) using the which function from aboce
      read.spatial$explicit.ChanTable$US_area_m2[reach.name.dynamic]               # Using the function from above
    
    # Convert contributing area to km2
    reach.contributing.area[name.logger,1] <- 
      reach.contributing.area[name.logger,1]*1e-6
    

    
    # Run the analysis to simulate the 1/0 time series for the four sensors over the 1000 transmissivity scaling exponents
    for (gamma.in in 1:length(gamma.range)) {                                              
      # Write the simulated 1/0 time series for the average threshold
      
      # Determine the subsurface flow capacity of the valley below the reach. This equation is proposed in Godsey and Kirchner (2018) and Prancevic and Kirchner (2014)
      transmissivity <- exp(gamma.range[gamma.in])
      thresh <- transmissivity*reach.slope[name.logger,1]
      
      #percent.thresh <- Flow.duration.reaches[which(r.names==Reach.identifiers[name.logger])][nearest(Q.in.rank[which(r.names==Reach.identifiers[name.logger])],         # Look at the flow duration for reach 810 (associated with FC4) and 
      #                                                                                                thresh),1]                            # Look at the flow duration for reach 810 (associated with FC4) and 
      explicit.r.names <- names(explicit.routing.run$routed.flow.kinematic.m3_hr)
      
      
      reach.identifiers.second <- paste0('R',reach.identifiers)
      
      sim.on.off <- 
        ifelse(explicit.routing.run$routed.flow.kinematic.m3_hr[which(explicit.r.names==reach.identifiers.second[name.logger])]>thresh,           # USing the iterative threshold
               1,0)                                                               # Set the values greater than the threshold to one, otherwise set to zero 
      sim.on.off <- data.frame(sim.on.off)                                           # Convert to a data frame
      #time.sim <- time(result.optim.run$run$fluxes$qin)                              # Get the time stamp for the qsim
      sim.on.off$Date <- time.sim$Date                                                    # Set the date of the data frame to the time.sim 
      
      # Compare the logger 1/0 to the simulated 1/0
      sim.obs.match <- left_join(logger.timeseries[[name.logger]],sim.on.off,by='Date')                           # Join the logger time series to the simulated time series by the date
      sim.obs.match$state <- as.numeric(sim.obs.match$state)                                   # Write the state as a numeric
      sim.obs.match[,3] <- as.numeric(sim.obs.match[,3])
      sim.obs.match$difference <- sim.obs.match$state-sim.obs.match[,3]                     # Subtract simulated state from the observed state. state is observed, order.1.on is simulated; if the difference is 0 then the sim and obs match. If it's 1 then the obs = 1 and sim = 0. If it's -1 then obs = 0 and sim = 1
      sim.obs.match$difference <- ifelse(sim.obs.match$difference==3|
                                           sim.obs.match$difference==2,  # If the difference is 3 or 2, then this should just be set to NA. I set observed NA values to 3 before, not sure why but it makes the code work.
                                         NA,sim.obs.match$difference)                          # If it's 3 or 2 then it will just be NA, otherwise it will be the correct difference.
      
      # Sum the number of correct, missed on, missed off values 
      zero.difference <- sum(sim.obs.match$difference==0,na.rm=T)                          # Zero values mean sim and obs state match; this is the sum of zero difference time steps
      true.on <- sum(ifelse(sim.obs.match$state==1 & sim.obs.match[,3]==1, # See if both obs and model equal 1
                            1,0),na.rm=T)                                                    # Calculate the true positives
      true.off <- sum(ifelse(sim.obs.match$state==0 & sim.obs.match[,3]==0,# See if both obs and model equal 0
                             1,0),na.rm=T)                                                   # Calculate the true negatives
      missed.on <- sum(sim.obs.match$difference==1,na.rm=T)                                # 1 means the obs = 1 and sim = 0; so the model missed the sensor being 'on' or 'wet' since 1 - 0 = 1
      missed.off <- sum(sim.obs.match$difference==-1,na.rm=T)                              # -1 means the obs = 0 and the sim = 1; so the model missed the sensor being 'off' or 'dry' since 0 - 1 = -1
      
      correct.state.percent.thresh <-                                                         # The correct percent is the total number of times when the difference is zero
        zero.difference/(zero.difference+missed.on+missed.off)*100                     # divided by the total number of timesteps when the sensor has data 
      
      flow.permanence.thresh <- sum(sim.obs.match[,3]==1)/length(sim.obs.match[,3])     # Flow permanence is the sum of time steps when the sensor is 'on' or 'wet' divided by the total number of time steps 
      
      if(gamma.in==1) {
        confusion.matrix.thresh[[name.logger]] <- data.frame(true.on=true.on,                         # Calc the confusion matrix
                                                             true.off=true.off,                      # True off  
                                                             missed.on=missed.on,                    # missed on
                                                             missed.off=missed.off)                  # missed off
      } else { 
        confusion.matrix.join.thresh <- data.frame(true.on=true.on,                    # Join to the previous matrix      
                                                   true.off=true.off,                       # true off
                                                   missed.on=missed.on,                     # missed on
                                                   missed.off=missed.off)                   # missed off
        confusion.matrix.thresh[[name.logger]] <- rbind(confusion.matrix.thresh[[name.logger]],                       # Join to the previous
                                                        confusion.matrix.join.thresh)                  # Just created join matrix
      }
      
      # Calculate the percent thresh 
      
      if (gamma.in==1) {
        out.thresh.df[[name.logger]] <- data.frame(percent.correct=correct.state.percent.thresh,
                                                   flow.thresh=thresh, sum.correct=sum(true.on,true.off), # percent.thresh=percent.thresh,
                                                   sum.incorrect=sum(missed.on,missed.off),
                                                   transmissivity = transmissivity)
      } else {
        out.thresh.bind <- data.frame(percent.correct=correct.state.percent.thresh,
                                      flow.thresh=thresh, sum.correct=sum(true.on,true.off), # percent.thresh=percent.thresh,
                                      sum.incorrect=sum(missed.on,missed.off),
                                      transmissivity = transmissivity)
        out.thresh.df[[name.logger]] <- rbind(out.thresh.df[[name.logger]],out.thresh.bind)
      }

    
      }
    
    
     residual.on.off[,name.logger] <- confusion.matrix.thresh[[name.logger]][,3]+
    confusion.matrix.thresh[[name.logger]][,4]

  }
  Sys.time()-x
  
  
  
  # Calculate the total residual error by summing the error for each sensor
  rowSums.residual.on.off <- data.frame(sum.error=rowSums(residual.on.off),transmissivity=exp(gamma.range))
  
  for (i in 1:length(names.loggers)) { 
    thresh.vals = data.frame(out.thresh.df[[i]]$flow.thresh)
    colnames(thresh.vals) <- paste0(reach.identifiers[i],' thresh')
    rowSums.residual.on.off <- cbind(rowSums.residual.on.off,thresh.vals)
    }
                                        
  
  # calculate which threshold produces the minimum error accross all four sensors
  thresh.best <- (rowSums.residual.on.off$transmissivity[which.min(rowSums.residual.on.off$sum.error)])
  
  best.residual.on.off <- rowSums.residual.on.off[which(rowSums.residual.on.off$transmissivity==thresh.best),]
  # Output results for all thresholds into a list
  flow.thresh.best.results <- 
    list(out.thresh.data=out.thresh.df, #sim.obs.difference=sim.obs.difference, 
         confusion.matrix=confusion.matrix.thresh,rowSums.residual.on.off=rowSums.residual.on.off)
  
  # 
  for (name.logger in 1:length(names.loggers)) {
    if (name.logger ==1) {
      save.out.thresh.best <- data.frame(out.thresh.df[[name.logger]][which(out.thresh.df[[name.logger]]$flow.thresh==thresh.best),])
    }else {
      save.out.thresh.best.bind <- out.thresh.df[[name.logger]][which(out.thresh.df[[name.logger]]$flow.thresh==thresh.best),]
      save.out.thresh.best <- rbind(save.out.thresh.best,save.out.thresh.best.bind)
    }
  }
  
  # Calculate the percent of time that the network is on and the percent of time the network is off
  # Also calcualte the time series of on/off 
  Q.sub.c.best <- thresh.best*read.spatial$explicit.ChanTable$stream_slope_ftpft
  total.network.on.off <- data.frame(matrix(nrow=nrow(explicit.routing.run$routed.flow.instant.m3_hr),ncol=ncol(explicit.routing.run$routed.flow.instant.m3_hr)))
  colnames(total.network.on.off) <- r.names
  for (reach in 1:length(Q.sub.c.best)) {
    total.network.on.off[,reach] <- ifelse(explicit.routing.run$routed.flow.instant.m3_hr[,reach]>Q.sub.c.best[reach],1,0)
  }
  
  flow.network.thresh <- explicit.routing.run$routed.flow.instant.m3_hr
  colnames(flow.network.thresh) <- r.names
  for (reach in 1:length(Q.sub.c.best)) {
    flow.network.thresh[flow.network.thresh[,reach] < (Q.sub.c.best[reach]),reach] <- 0
  }
  
  
  # Calculate the percent of time that each reach is considered to be on
  reach.names.list <- colnames(total.network.on.off)
  
  percent.on.network <- c()
  for (name.iter in 1:length(reach.names.list)) {
    percent.on.network[name.iter] <- 
      sum(total.network.on.off[,name.iter])/length(total.network.on.off[,name.iter])
  }
  
  percent.on.network <- data.frame(percent.on.network)
  rownames(percent.on.network) <- reach.names.list
  
  #best.percent.thresh <- out.thresh.df[[4]]$percent.thresh[which(out.thresh.df[[4]]$flow.thresh==(thresh.best))]
  
  # Write the simulated 1/0 time series for the best threshold
  #order.1.on <- 
  #  ifelse(explicit.routing.run$routed.flow.instant.m3_hr$`810`>thresh.best, # USing the averaged threshold
  #         1,0)                                                                    # Set the values greater than the threshold to one, otherwise set to zero 
  #order.1.on <- data.frame(order.1.on)                                             # Convert to a data frame
  #time.sim <- time(result.optim.run$run$fluxes$qin)                                # Get the time stamp for the qsim
  #order.1.on$Date <- time.sim                                                      # Set the date of the data frame to the time.sim 
  
  # Compare the logger 1/0 to the simulated 1/0
  #final.match <- left_join(order.1.on,logger.timeseries[[4]],by='Date')                           # Join the logger time series to the simulated time series by the date
  #final.match$state <- as.numeric(final.match$state)                                   # Write the state as a numeric
  #final.match$difference <- final.match$state-final.match$order.1.on                     # Subtract simulated state from the observed state. state is observed, order.1.on is simulated; if the difference is 0 then the sim and obs match. If it's 1 then the obs = 1 and sim = 0. If it's -1 then obs = 0 and sim = 1
  #final.match$difference <- ifelse(final.match$difference==3|final.match$difference==2,  # If the difference is 3 or 2, then this should just be set to NA. I set observed NA values to 3 before, not sure why but it makes the code work.
  #                                 NA,final.match$difference) 
  
  # Sum the number of correct, missed on, missed off values for 2003 
  #zero.difference.yr2003 <- sum(final.match$difference[2218:4473]==0,na.rm=T)        # This is the time when the sensor is actively collecting data between 2003-2004; sum the values equal to 0
  #missed.on.yr2003 <- sum(final.match$difference[2218:4473]==1,na.rm=T)              # Sum the values equal to 1
  #missed.off.yr2003 <- sum(final.match$difference[2218:4473]==-1,na.rm=T)            # Sum the values equal to 0
  #correct.state.percent.yr2003 <- zero.difference.yr2003/                          # Calculate correct percent for 2003
  #  (zero.difference.yr2003+missed.on.yr2003+missed.off.yr2003)*100                # number of time steps equal to zero divided by total number of timesteps for 2003
  #flow.permanence.yr2003.sim <-                                                    # Simulated 2003 'on' divided by total 
  #  sum(final.match$order.1.on[2218:4473]==1)/length(final.match$order.1.on[2218:4473])# 2003 time steps
  #flow.permanence.yr2003 <-                                                        # observed 2003 fraction 'on' 
  #  sum(final.match$state[2218:4473]==1,na.rm=T)/                                    # Total 1 divided by 
  #  ((sum(final.match$state[2218:4473]==1,na.rm=T))+                                 # Total 1 plus
  #     (sum(final.match$state[2218:4473]==0,na.rm=T)))                               # Total 0
  
  # Sum the number of correct, missed on, missed off values for 2005
  #zero.difference.yr2005 <- sum(final.match$difference[10135:14323]==0,na.rm=T)      # This is the time when the sensor is actively collecting data between 2005-2006; sum the values equal to 0
  #missed.on.yr2005 <- sum(final.match$difference[10135:14323]==1,na.rm=T)            # Sum the values equal to 1
  #missed.off.yr2005 <- sum(final.match$difference[10135:14323]==-1,na.rm=T)          # Sum the values equal to -1
  #correct.state.percent.yr2005 <- zero.difference.yr2005/                            # Calculate correct percent for 2005
  #  (zero.difference.yr2005+missed.on.yr2005+missed.off.yr2005)*100                  # number of time steps equal to zero divided by total number of time steps
  #flow.permanence.yr2005.sim <-                                                      # Simulated 2005 'on' divided by 
  #  sum(final.match$order.1.on[10135:14323]==1)/                                     # total 2005 simulated
  #  length(final.match$order.1.on[10135:14323])                                      # timesteps
  #flow.permanence.yr2005 <-                                                          # Observed 2005 fraction 'on'  
  #  sum(final.match$state[10135:14323]==1,na.rm=T)/                                  # total 1 divided by 
  #  ((sum(final.match$state[10135:14323]==1,na.rm=T))+                               # Total 1 plus
  #     (sum(final.match$state[10135:14323]==0,na.rm=T)))                             # Total 0
  
  # Calculate statistics for the flow exceeded, correct state percent for the total period, 2003, and 2005, flow permanence
  #stats.on.off <- data.frame(                                          # Write flow exceeded and correct.state.percent
  #  correct.percent.2003=correct.state.percent.yr2003, correct.percent.2005 = correct.state.percent.yr2005,               # Write the correct state for 2003 and 2005
  #  flow.permanence.2003.sim=flow.permanence.yr2003.sim,                           # Write the obs and sim flow permanences
  #  flow.permanence.2005.sim=flow.permanence.yr2005.sim,flow.permanence.2003=flow.permanence.yr2003,                      # Write the obs and sim flow permanences
  #  flow.permanence.2005=flow.permanence.yr2005) #best.percent.exceeded = best.percent.threshold,                                                                          # Write the obs and sim flow permanences
  
  # Compile everything as a list
  out.headwater.evaluation <- list(                                       # Compile a list of everything we want to output
    flow.thresh.best.results=flow.thresh.best.results, # stats.on.off=stats.on.off,
    thresh.best=thresh.best, #best.percent.thresh=best.percent.thresh,
    total.network.on.off=total.network.on.off,
    flow.network.thresh=flow.network.thresh, percent.on.network=percent.on.network, 
    save.out.thresh.best= save.out.thresh.best,
    best.residual.on.off=best.residual.on.off,
    Q.sub.c.best=Q.sub.c.best
  )                                                                                      # Continued
  
  return(out.headwater.evaluation)                                                 # return the list and exit the function
}
```


# Step 6: plot the data 

```{r}
# Read the optimal parameter values and rerun the model                                
read.out.calib <- readRDS('D:/Data/Dynamic-TOPMODEL/Little-Millseat/Calibration-1/dynatop_calib_LM.RData')

optim.params <- read.out.calib$par                                                            # Read the optimal parameter set
names(optim.params) <- names(params)                                               # Set names 
result.optim <- runPSOCalibrationTopmodel(param.values=optim.params,               # Run the model with the optimal parameter set
                                          inner.timesteps = in.timestep,           # input the inner.timesteps
                                          rains = rain.calib, PETs=PET.calib,      # All forcing components are the same
                                          obss=Q.obs.calib, discs=disc,            # Spatial information comes from dynatop spatial function
                                          RoutingTables=NULL,                      # See above
                                          dates.dfs = dates.df)                    # Calibration end date input
sim.zoo <- (window(result.optim$run$qsim, start = dates.df$calibration.initial,         # window the simulation
                         end = dates.df$calibration.final))                        # Will convert to zoo obj later
obs.zoo <- (window(result.optim$run$qobs, start = dates.df$calibration.initial,              # Window the observed
                  end = dates.df$calibration.final))                               # Will convert to zoo obj later



sim.zoo <- zoo(sim.zoo)                                         # Convert to zoo object
obs.zoo <- zoo(obs.zoo)                                         # Convert to zoo object

plot(sim.zoo)
lines(obs.zoo,col='red')


PSO.drty <- "/PSO.out/"                             # Specify the directory where all of the PSO files are written to
res <- read_results( MinMax="max",            # Read the results of the calibration; MAX is noting that we are looking for maximum KGE vals
                    beh.thr=0.3,modelout.cols=NULL)             # 0.3 is the threshold for specifying behavioral model realizations
params <- res[["params"]]                                       # Record the optimal parameters from the results
gofs <- res[["gofs"]]                                           # Read the goodness of fit values
Qsims <- res[["model.values"]]                                  # Model values for the simulations
model.best <- res[["model.best"]]                               # Read the best model results
model.obs <- res[["model.obs"]]                                 # Outputthe observed results 
optim.params <- data.frame(t(res$best.param))                   # Transpose the data and write as dataframe to be read in by the model                                                                   # print stats

## Process Quantiles of the model results 
params.025.50.975 <- wquantile(params, weights=gofs,            # Determine the weighted quantiles based on the gofs
                               byrow=FALSE,                     # Specifying wquanitle inputs
                               probs=c(0.025, 0.5, 0.975),      # probabilities to be calculated
                               normwt=TRUE, verbose=FALSE)      # Other inputs
params.025.50.best.975 <- cbind(params.025.50.975[, c(1,2)],    # Cbind to determine how far past the median
                                Best=as.numeric(optim.params[[1]]),                # This is realated to the beset parameters
                                params.025.50.975[, 3] )        # Which column is needed 
colnames(params.025.50.best.975)[4] <- "97.5%"                  # Column names
round( params.025.50.best.975, 2)                               # Rounding the parameters - for viewing
n <- ncol(Qsims)                                                # number of time steps
Qsim.025.q50.q975 <- wquantile(Qsims, weights=gofs, byrow=FALSE,# This is based on the Qsims this time 
                               probs=c(0.025, 0.5, 0.975),      # This is the quantiles that will be calculated
                               normwt=TRUE, verbose=FALSE)      # It's normalized
#round( head(Qsim.025.q50.q975), 3)                              # Prints the quantiles Qsim
q025 <- zoo(Qsim.025.q50.q975[,1], dates.cal)                   # Transform the 2.5 quantiles to zoo
q975 <- zoo(Qsim.025.q50.q975[,3], dates.cal)                   # Transform the 97.5 quantile to zoo

pf <- pfactor(x=obs.zoo, lband=q025, uband=q975, na.rm=TRUE)# Calculate the P factor
pf                                                              # Print the P factor
rf <- rfactor(x=obs.zoo, lband=q025, uband=q975, na.rm=TRUE)# Calculate the R factor
rf                                                              # Print the R factor 

# Convert from m/hr (the output of dynatopmmodel) to mm/day: 1 m / 1 hr * 1000 mm / 1 m * 24 hr / 1 day
sim.plot <- sim.zoo*24000
colnames(sim.plot) <- 'sim'
obs.plot <- obs.zoo*24000
colnames(obs.plot) <- 'obs'
lower.plot <- q025*24000
upper.plot <- q975*24000

obs.minus.sim.plot <- obs.plot-sim.plot
colnames(obs.minus.sim.plot) <- 'obs.minus.sim.plot'
outlet.sim.obs <- data.frame(time=as.Date(time(obs.zoo),format='%y-%d-%m %hh:%mm:%ss'),'sim'=sim.plot,'obs'=obs.plot,
                             'lower'=lower.plot,'upper'=upper.plot,'obs.minus.sim'=obs.minus.sim.plot) # This code converts everything from m/hr to mm/day

sim.obs.outlet.plot <- ggplot(data=outlet.sim.obs,aes(x=time,y=sim))+geom_ribbon(data=outlet.sim.obs,aes(ymin=lower,ymax=upper),fill='grey70')+geom_line(data=outlet.sim.obs,aes(x=time,y=obs),color='red',linetype='solid') +theme(legend.position = "none")
sim.obs.outlet.plot <- sim.obs.outlet.plot+geom_line(color='black',size=.55)+ theme_bw() + xlab('Date') +
  scale_x_date(date_breaks = '6 months')+ylab('Discharge (mm/d)*')+ylim(0,20.0)
sim.obs.outlet.plot <-  sim.obs.outlet.plot + theme(axis.title.x=element_blank())
sim.obs.outlet.plot

sim.obs.window <- outlet.sim.obs[which(rownames(outlet.sim.obs)=="2004-09-01 00:00:00"):which(rownames(outlet.sim.obs)=="2004-11-01 00:00:00"),]   

sim.obs.outlet.plot.2 <- ggplot(data=sim.obs.window,aes(x=time,y=sim))+geom_ribbon(data=sim.obs.window,aes(ymin=lower,ymax=upper),fill='grey70')+geom_line(data=sim.obs.window,aes(x=time,y=obs),color='red',linetype='solid') +theme(legend.position = "none")
sim.obs.outlet.plot.2 <- sim.obs.outlet.plot.2+geom_line(color='black',size=.55)+ theme_bw() + xlab('Date') +ylab('Discharge (mm/d)*')+ylim(0,20.0)
sim.obs.outlet.plot.2 <-  sim.obs.outlet.plot.2 + theme(axis.title.x=element_blank())
sim.obs.outlet.plot.2

plot(sim.obs.window$sim)
```
Okay good... the model works with the new spatial discretization. 

Now on to the hydromet inputs... 

```{r}
if(length(run$Qsim)==12502) {
    perc.diff.Qsim.explicit <- as.vector((run$Qsim-rowSums(explicit.chan.inputs.qbf[1:length(run$Qsim),1:33]))/run$Qsim*100)
    
    check.balances <- data.frame(explicit=rowSums(explicit.chan.inputs.qbf[1:length(run$Qsim),1:33]),
                                 Qsim=run$Qsim,
                                 diff.explicit.Qsim = rowSums(explicit.chan.inputs.qbf[1:length(run$Qsim),1:33]) -
                                   run$Qsim,
                                 qin=lumped.qin[1:length(run$Qsim),1],
                                 diff.qin.Qsim = lumped.qin[1:length(run$Qsim),1]-
                                   run$Qsim,
                                 perc.diff.Qsim.explicit=perc.diff.Qsim.explicit)
    
  } else {
    perc.diff.Qsim.explicit <- as.vector((run$Qsim[1:nrow(run$fluxes$qbf)]-rowSums(explicit.chan.inputs.qbf[,1:33]))/run$Qsim[1:nrow(run$fluxes$qbf)]*100)
    check.balances <- data.frame(explicit=rowSums(explicit.chan.inputs.qbf[,1:33]),
                                 Qsim=run$Qsim[1:nrow(run$fluxes$qbf)],
                                 diff.explicit.Qsim = rowSums(explicit.chan.inputs.qbf[,1:33]) -
                                   run$Qsim[1:nrow(run$fluxes$qbf)],
                                 qin=lumped.qin[,1],
                                 diff.qin.Qsim = lumped.qin[,1]-
                                   run$Qsim[1:nrow(run$fluxes$qbf)],
                                 perc.diff.Qsim.explicit=perc.diff.Qsim.explicit)
  }
```

